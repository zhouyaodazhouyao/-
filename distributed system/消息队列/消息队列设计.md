## PMQ消息队列设计

### 1.设计目标

- 简单
- 不丢消息，日亿级消费存储能力，按需重复消费
- 高性能，平均秒级延迟，吞吐量要高，单机每秒消费1000是底线
- 可扩展，支持简单水平扩展（简单增加数据库和broker）
- 隔离性，不同业务团队主题分区要相互隔离 ，消息生产和消费方相互隔离互不影响
- 监控治理，对消息发送、消费、堆积问题（**lag堆积监控**）能够及时发现并告警

```
kafka设计限制
1.消息顺序性
**能保证消息顺序写入，保证相同分区的消息是顺序的（排除网络延迟），但是多个分区之间可能是乱序的
**消息多线程并行消费或者多个分区并行消费，消息消费顺序可能是乱序的
2.消费语义
**消费者挂掉或者重启，如果没有及时提交偏移量，重启后会接受到少量重复消息，消费者端业务方要做幂等处理
```



### 2.总体架构设计

##### Queue和Topic语义

前者是一个队列只支持一个消费者，后者是一个队列支持多个消费者，且消费者之间互不相关，Topic语义是kafka的consumerGroup的基础

##### 核心概念模型

![concept model](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/concept model.png)

##### 存储设计

下图为消息队列存储的抽象模型

![memory design](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/memory design.png)

具体实现来说

**kafka：**dataNode是由Broker服务器节点直接承担的，partition是通过磁盘文件来实现的，一个partition对应一个磁盘文件，broker上的partition数量没有限制，一般建议不超过4000

**PMQ：**使用MySQL存储队列，每个数据库实例对应一个DataNode，每个实例当中的表对应一个partition。

##### 元数据模型

![metadata model](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/metadata model.png)

##### 总体设计

<img src="/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/overall design.png" alt="overall design"  />

[^Broker推or消费者拉？]: 拉模式可以让Broker无状态，易于水平扩展，同时消费者端具有更好的隔离性和吞吐量，拉模式是大规模企业级消息队列设计的最佳实践

##### Producer端

![producer design](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/producer design.png)

[^注]: kafka的负载均衡做在Producer端，由于PMQ是使用MySQL的，和Broker是分离的，所以负载均衡在Broker端

##### Consumer端

**关于动态重平衡**：一旦消费者组内有数量变更，会自动将分区和消费者之间的指向关系重新调整。因而，拥有动态重平衡算法的消费者组，不担心组内消费者会挂掉，也不用考虑新加消费者带来的问题。（**具体协议后续补充**）

![consumer design](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/consumer design.png)

PMQ没有采用动态重平衡的复杂设计，相应地使用了简单分区竞争策略，一个消费者消费一个分区，同时最好配置一个备用消费者，一旦其他消费者挂掉，能及时顶上去。

[^注]: 这里的consumer不需要单独机器部署，一个机器上可以部署多个consumer进程，因为和分区绑定的是消费者的进程号

![consumer api design](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/消息队列/consumer api design.png)

### 3.PMQ和Kafka的比较

|                | PMQ                   | Kafka                      |
| -------------- | --------------------- | -------------------------- |
| 队列持久化     | MySQL                 | 文件                       |
| 通讯层         | Thrift                | 定制NIO协议                |
| 元数据分区管理 | MySQL+静态分配        | Zookeeper动态管理          |
| 消费者负载均衡 | 通过IP+进程号竞争分配 | 动态重平衡                 |
| 消费状态存储   | 客户端+MySQL          | 客户端+Zookeeper或Broker   |
| 消息高可用     | MySQL高可用           | 不同Broker上存多份消息拷贝 |



#### 存什么？

1.单个事件（每次点击）

| VideoId | Timestamp           | ...  |
| ------- | ------------------- | ---- |
| A       | 2020-05-21 16:17:21 | ...  |
| A       | 2020-05-21 16:17:32 | ...  |
| A       | 2020-05-21 16:17:40 | ...  |
| B       | 2020-05-21 16:22:47 | ...  |

**优点**：可以快速写入；按需聚合运算

**缺点**：查询慢；耗存储



2.聚合数据（例如每分钟）

| VideoId | Timestamp        | count |
| ------- | ---------------- | ----- |
| A       | 2020-05-21 16:17 | 3     |
| B       | 2020-05-21 16:22 | 1     |

**优点**：查询快；存储少

**缺点**：只能按已聚合方式查询；需实时聚合运算；出错修复无法简单修复

#### 数据库选型

主要基于非功能需求

- **可扩展**——根据读写规模按需扩展
- **高性能**——快速读写
- **高可用**——不丢数据，灾难恢复
- **一致性折衷**
- 数据模型易于升级
- 成本
- 开发者学习门槛

##### 1.SQL数据库+客户端嵌入代理

![count-database-1](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/count-database-1.png)

为了避免sharding和主从复制引入的复杂性，可以引入数据库访问代理，以客户库的形式嵌入到服务当中。（**ShardingSphere**）

**缺点**：引入了很多复杂性，包括手工sharding、引入主从复制机制、引入数据库访问代理和配置中心、Resharding人工导入数据等

##### 2.NoSql数据库（Cassandra）

![count-database-nosql](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/count-database-nosql.png)

节点对等，节点之间定期交换信息（**gossip协议**），哈希算法使用的是**一致性哈希算法**；协调者可以按照某种选举方式选出来；读取和写入都直接经过协调者，读取使用仲裁读（Quorum read：多数返回结果一致即返回），写入由协调者判断数据属于哪一个节点，然后写入该节点中，同时为了高可用，会复制几份（**复制因子（Replication）**可配置）到其他节点，使用仲裁写（Quorum write：多数节点写入成功即视为成功）。

**优点**：线性扩容，无需人工干预；支持跨数据中心

**缺点**：运维管理复杂

#### 表设计

**1.SQL数据库**

| VideoId | Name   | ...  | SpaceId |
| ------- | ------ | ---- | ------- |
| X       | 马保国 | ...  | 123     |

Video_Info表

| VideoId | Timestamp | count |
| ------- | --------- | ----- |
| X       | 15:00     | 2     |
| X       | 16:00     | 3     |
| X       | 17:00     | 6     |

Video_status表

| SpaceId | Name   | ...  |
| ------- | ------ | ---- |
| 123     | 马保国 | ...  |

Space_Info表

**2.NoSQL数据库**

| VideoId+Date | SpaceName | VideoName | 15:00 | 16:00 | 17:00 | ...  |
| ------------ | --------- | --------- | ----- | ----- | ----- | ---- |
| X-2020.05.21 | 马保国    | 马保国    | 2     | 3     | 6     | ...  |

### 4.计数服务的实现

- **可扩展**：Partitioning/Sharding
- **高性能吞吐**：内存计算，Batch批处理
- **高可靠**：持久化，Replication，checkpointing

![count-service](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/count-service.png)

[^图1]: counting consumer 详细设计

![Data-Ingestion-Path](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/Data-Ingestion-Path.png)

[^图2]: 数据接收路径

### 5.查询服务的实现

![query-service](/Users/shuuyou/just-for-review-and-zhuangB/distributed system/query-service.png)

### 6.技术栈选型

- Api Gateway：NetFlix Zuul网关 / Spring Cloud Gateway
- Api前置负载均衡器：F5 / SLB负载均衡器（阿里云）
- Counting Service / Query Service / Counting Consumer：Spring Boot
- 服务发现：NetFlix Eureka
- 客户端软负载：NetFlix Ribbon
- 缓存：Redis
- MQ：Kafka
- 存储数据：Cassandra / HBase
- 线下计算：Hadoop / 对象存储OSS（阿里云）

### 7.进一步考量和总结

- **如何定位系统瓶颈？**

  性能和压力测试，找出潜在的性能、内存和多线程问题，同时在基础性能基础上进行性能调优；还可以对生产部署进行容量估算，以确定申请多少软硬件资源

- **如何监控系统健康状况？**

  细粒度埋点监控，对核心服务的调用量，调用延迟和错误数都要监控起来；硬件资源需监控的有CPU、内存和磁盘的利用率；如果使用java语言，JVM的垃圾回收也要监控；MQ的消息堆积情况；

- **如何监控慢消费？解决？**

  消息队列设计中有解释

- **如何解决热分区问题？**

  添加时间戳，按照每小时分摊到不同分区

分布式基本概念

功能需求、非功能需求、读写路径、分布式数据存储、sharding/replication、消息队列、消费者、负载均衡、服务发现、缓存、流式计算